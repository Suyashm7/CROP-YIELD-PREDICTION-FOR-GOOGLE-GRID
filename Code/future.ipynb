{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from keras import losses\n",
    "\n",
    "custom_objects = {\n",
    "    'mse': losses.mean_squared_error\n",
    "}\n",
    "\n",
    "model = load_model(\n",
    "    r'C:/Users/sahil/Desktop/Crop_yeild/blstm_100_future/BLSTM_7_crop_yield_prediction_20.h5',\n",
    "    custom_objects=custom_objects\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\sahil\\desktop\\crop_yeild\\.venv\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sahil\\desktop\\crop_yeild\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sahil\\desktop\\crop_yeild\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.6 MB 4.6 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/11.6 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/11.6 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.2/11.6 MB 6.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.6 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.9/11.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.6 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\sahil\\desktop\\crop_yeild\\.venv\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/11.1 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.4/11.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.5/11.1 MB 7.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.0/11.1 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.1/11.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.7/11.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.0/11.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 6.6 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp310-cp310-win_amd64.whl (41.2 MB)\n",
      "   ---------------------------------------- 0.0/41.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/41.2 MB 7.7 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.9/41.2 MB 7.0 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 4.2/41.2 MB 6.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 5.5/41.2 MB 6.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 6.6/41.2 MB 6.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 8.1/41.2 MB 6.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 9.2/41.2 MB 6.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 10.7/41.2 MB 6.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 11.8/41.2 MB 6.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 13.1/41.2 MB 6.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.7/41.2 MB 6.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 16.0/41.2 MB 6.5 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 17.0/41.2 MB 6.4 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 18.6/41.2 MB 6.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 19.9/41.2 MB 6.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 21.0/41.2 MB 6.4 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 22.5/41.2 MB 6.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 23.9/41.2 MB 6.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 25.2/41.2 MB 6.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 26.5/41.2 MB 6.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 27.8/41.2 MB 6.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 29.1/41.2 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 30.4/41.2 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 31.7/41.2 MB 6.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 33.0/41.2 MB 6.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 34.3/41.2 MB 6.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 35.7/41.2 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 37.2/41.2 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 38.5/41.2 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.6/41.2 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.9/41.2 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.2/41.2 MB 6.3 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file2 = 'C:/Users/sahil/Desktop/Crop_yeild/maize_country_data_20/maize_India.csv'\n",
    "\n",
    "\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "\n",
    "\n",
    "# Backup the original 'crop', 'State', and 'Country' columns\n",
    "original_columns = df2[['crop', 'State', 'Country']].copy()\n",
    "# Check for duplicates and remove them if needed\n",
    "df2 = df2.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "df2.drop(['crop', 'State', 'Country'], axis=1, inplace=True)\n",
    "#remove the columns with missing data\n",
    "df2.dropna(inplace=True)\n",
    "#remove the variable with VIF greater than a threshold from the data set\n",
    "#df.drop(['Tmin','Tmax', 'Humidity_min',  'Humidity_max','Solar R_Avg'],axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def data_split(df, train=True):\n",
    "    sequential_data = []\n",
    "    season = []\n",
    "    for index, i in enumerate(df.values):\n",
    "        season.append(i[:-1].tolist())  # All columns except the last one (yield)\n",
    "        if len(season) == 240:\n",
    "            sequential_data.append([np.array(season), i[-1]])  # Last column is the target (yield)\n",
    "            season.clear()\n",
    "\n",
    "    if train:\n",
    "        random.shuffle(sequential_data)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        Y.append(target)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split the data for supervised learning using the data_split function\n",
    "X, Y = data_split(df2)\n",
    "indices = np.arange(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the pre-fitted scaler\n",
    "scaler = joblib.load('minmax_scaler.save') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m349/349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 120ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Concatenate X and Y to preprocess test set\n",
    "X_reshape = np.reshape(X, (X.shape[0] * X.shape[1], X.shape[2]))\n",
    "Y_reshape = np.concatenate((Y.reshape(-1, 1), np.zeros((X.shape[0] * X.shape[1] - len(Y), 1))), axis=0)\n",
    "data_test = np.concatenate((X_reshape, Y_reshape), axis=1)\n",
    "\n",
    "# Normalize the test set using the training MinMaxScaler\n",
    "data_test_scaled = scaler.transform(data_test)\n",
    "X_scaled = data_test_scaled[:, :-1]\n",
    "X_scaled = np.reshape(X_scaled, (X.shape[0], X.shape[1], X.shape[2]))\n",
    "\n",
    "# Predict on the test set\n",
    "yhat = model.predict(X_scaled)\n",
    "\n",
    "# Denormalize the predictions and actual values\n",
    "# Append predictions to test input for inverse transformation\n",
    "yhat_reshape = np.concatenate((yhat, np.zeros((X.shape[0]*X.shape[1] - len(yhat), yhat.shape[1]))), axis=0)\n",
    "data_pred = np.concatenate((X_reshape, yhat_reshape), axis=1)\n",
    "\n",
    "# Inverse transform both true and predicted data\n",
    "data_actual = scaler.inverse_transform(data_test_scaled)\n",
    "data_pred = scaler.inverse_transform(data_pred)\n",
    "\n",
    "inv_y = data_actual[:len(Y), -1]\n",
    "inv_yhat = data_pred[:len(Y), -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04234587],\n",
       "       [0.15919495],\n",
       "       [0.13366672],\n",
       "       ...,\n",
       "       [0.07410336],\n",
       "       [0.07415499],\n",
       "       [0.08805815]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score       = -2.4609\n",
      "Mean Squared Error (MSE) = 122.2460\n",
      "Mean Absolute Error (MAE) = 9.3737\n",
      "Root Mean Squared Error (RMSE) = 11.0565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "# Calculate R², MSE, MAE, RMSE\n",
    "r2 = r2_score(inv_y, inv_yhat)\n",
    "mse = mean_squared_error(inv_y, inv_yhat)\n",
    "mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# Print results\n",
    "print(f'R² Score       = {r2:.4f}')\n",
    "print(f'Mean Squared Error (MSE) = {mse:.4f}')\n",
    "print(f'Mean Absolute Error (MAE) = {mae:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE) = {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.10891 , 18.98763 ,  7.149752, ...,  9.964984, 18.32651 ,\n",
       "       14.62276 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Actual  Predicted\n",
      "0  13.108910   0.306203\n",
      "1  18.987630   1.151139\n",
      "2   7.149752   0.966544\n",
      "3  13.787640   4.452828\n",
      "4   3.965434   2.900227\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for actual vs predicted values\n",
    "datapredicted = pd.DataFrame({\n",
    "    'Actual': inv_y,\n",
    "    'Predicted': inv_yhat\n",
    "})\n",
    "\n",
    "# Print and save to CSV\n",
    "print(datapredicted.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset with predictions saved as 'updated_with_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Flatten the predicted values to match the original data's shape\n",
    "predicted_flattened = np.repeat(inv_yhat, 240)  # Repeat each prediction for 240 days\n",
    "\n",
    "# Ensure the length matches the original dataframe\n",
    "if len(predicted_flattened) > len(df2):\n",
    "    predicted_flattened = predicted_flattened[:len(df2)]\n",
    "else:\n",
    "    predicted_flattened = np.pad(predicted_flattened, (0, len(df2) - len(predicted_flattened)), 'constant', constant_values=np.nan)\n",
    "\n",
    "# Add predicted values to the dataframe\n",
    "df2['Predicted_Yield'] = predicted_flattened\n",
    "\n",
    "# Set Predicted_Yield to 0 where day is not equal    to 240\n",
    "df2['Predicted_Yield'] = np.where(df2['Day'] != 239, 0, df2['Predicted_Yield'])\n",
    "\n",
    "# Add original columns back to df2 with predicted_yield included\n",
    "df2_with_predictions = pd.concat([df2, original_columns], axis=1)\n",
    "# Save the updated dataframe with predictions\n",
    "df2_with_predictions.to_csv(\"updated_with_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Updated dataset with predictions saved as 'updated_with_predictions.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
